# BERT-Chinese-CS
针对中文计算机科技文献的预训练模型(基于国图分类，筛选所有属于TP3的文献)。

由于项目需要（专业文本的自然语言处理），在维普提供海量科技文献文本，重庆市科学院提供算力支持的情况下，训练一个Domain Specific的预训练模型。

资源 | 统计 |
 -|-
 GPU | TeslaV100 x 3
期刊数| 943
文章数| 
句子数| 

### 数据准备

使用维普提供的2000-2019年全国核心期刊数据(仅摘要), 将文本分句后写入文件，格式如下：

```
卷积神经网络在单标签图像分类中表现出了良好的性能,但是,如何将其更好地应用到多标签图像分类仍然是一项重要的挑战。
本文提出一种基于卷积神经网络并融合注意力机制和语义关联性的多标签图像分类方法。
首先,利用卷积神经网络来提取特征;其次,利用注意力机制将数据集中的每个标签类别和输出特征图中的每个通道进行对应;最后,利用监督学习的方式学习通道之间的关联性,也就是学习标签之间的关联性。
实验结果表明,本文方法可以有效地学习标签之间语义关联性,并提升多标签图像分类效果。

提出一种基于知识图谱的通联特征挖掘方法,为电信欺诈案件相关的数据分析及线索挖掘提供技术支持.
基于仿真的通话数据和电信欺诈案件数据,在分布式图数据库中构建知识图谱.
在此基础上使用图遍历及图算法、混合高斯模型,从联系链路、必要人物、核心人物的发现以及社会关系识别这4个维度进行分析挖掘.
在混合高斯模型中,提取9个关键通话特征,从通话模式聚类的角度来识别不同的社会关系.通过实验证明,图遍历及图算法能为电信欺诈人员和团伙的发现提供重要线索.
混合高斯模型识别出了5类社会关系,并且发现涉案人员之间的通话模式具有一定的特殊性,即通话次数多且多发生在凌晨,通话时间较长且保持联系的时间较长.
```

每篇文章的摘要分句，摘要之间间隔一行。


### 选择Pre-train模型

- [ ] BERT
- [x] ALBERT  
- [ ] ELECTRA

基本上取决于有多少算力，如果租得起TPU还是考虑一下这几个模型的large版。由于只有3块V100，所以不考虑base以上的模型。

#### 选择ALBERT的原因
[albert_zh](https://github.com/brightmart/albert_zh) 做过大规模中文数据的训练，并且其tiny版本在性能与BERT base接近的情况下，更加轻量级。

#### 为什么不使用ELECTRA
我们做科技文献类文本的预训练前期主要用于NER、RE等信息抽取任务，ELECTRA已经过部分研究者测试在该类任务上效果并不好。

